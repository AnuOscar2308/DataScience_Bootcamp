# -*- coding: utf-8 -*-
"""Bootcamp_Assignment_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Gf_SOPykG21Vjtwv62be6rxtP4WHw08
"""

#Linear Regression
#1. Preprocess Test data and get predictions
#2. Compute Mean Abolute Error, Mean Square error for test data
#3. Implement Ridge and Lasso Regression and then compute the following metrics on test data

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_absolute_error, mean_squared_error

data = pd.read_csv("train (1).csv")

data.shape

data.head()

data = data.drop(columns=['id', 'timestamp','country'])

data.columns

data.loc[data['hours_per_week'].isna(), 'hours_per_week'] = data['hours_per_week'].median()
data.loc[data['telecommute_days_per_week'].isna(), 'telecommute_days_per_week'] = data['telecommute_days_per_week'].median()

data = data.dropna()

data.info()

y = data['salary']
X = data.drop(columns=['salary'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
print("Training Set Dimensions:", X_train.shape)
print("Validation Set Dimensions:", X_test.shape)

num_cols = ['job_years','hours_per_week','telecommute_days_per_week']
num_cols

scaler = StandardScaler()
scaler.fit(X_train[num_cols])
X_train[num_cols] = scaler.transform(X_train[num_cols])

X_train

reg=LinearRegression()
reg.fit(X_train, y_train)

mean_absolute_error(y_train,reg.predict(X_train))

mean_squared_error(y_train,reg.predict(X_train))**0.5

import numpy as np
import pandas as pd
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Example: Loading data from a CSV file
data = pd.read_csv('train (1).csv')

# Separate features and target variable
X = data.drop('salary', axis=1)  # Replace 'target_column' with your actual target column name
y = data['salary']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train Ridge Regression model
ridge_model = Ridge(alpha=1.0)  # alpha is the regularization strength
ridge_model.fit(X_train, y_train)

# Initialize and train Lasso Regression model
lasso_model = Lasso(alpha=0.1)  # alpha is the regularization strength
lasso_model.fit(X_train, y_train)

ridge_predictions = ridge_model.predict(X_test)
lasso_predictions = lasso_model.predict(X_test)

# Compute metrics for Ridge Regression
ridge_mae = mean_absolute_error(y_test, ridge_predictions)
ridge_mse = mean_squared_error(y_test, ridge_predictions)
ridge_r2 = r2_score(y_test, ridge_predictions)

# Compute metrics for Lasso Regression
lasso_mae = mean_absolute_error(y_test, lasso_predictions)
lasso_mse = mean_squared_error(y_test, lasso_predictions)
lasso_r2 = r2_score(y_test, lasso_predictions)

# Print metrics
print("Ridge Regression:")
print(f"MAE: {ridge_mae}, MSE: {ridge_mse}, R-squared: {ridge_r2}\n")

print("Lasso Regression:")
print(f"MAE: {lasso_mae}, MSE: {lasso_mse}, R-squared: {lasso_r2}")

#Trees
#Compute errors on test sets
#Play with different parameter of decision trees and random forests and see the impact on train and test error
#[OPTIONAL] implement cross validation and get best hyperparameters

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

data = pd.read_csv('train (1).csv')
X = data.drop('salary', axis=1)
y = data['salary']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

dt_model = DecisionTreeRegressor(max_depth=5)
dt_model.fit(X_train, y_train)
dt_train_predictions = dt_model.predict(X_train)
dt_test_predictions = dt_model.predict(X_test)
rf_model = RandomForestRegressor(n_estimators=100, max_depth=5)
rf_model.fit(X_train, y_train)
rf_train_predictions = rf_model.predict(X_train)
rf_test_predictions = rf_model.predict(X_test)

dt_train_mae = mean_absolute_error(y_train, dt_train_predictions)
dt_test_mae = mean_absolute_error(y_test, dt_test_predictions)
dt_train_mse = mean_squared_error(y_train, dt_train_predictions)
dt_test_mse = mean_squared_error(y_test, dt_test_predictions)

rf_train_mae = mean_absolute_error(y_train, rf_train_predictions)
rf_test_mae = mean_absolute_error(y_test, rf_test_predictions)
rf_train_mse = mean_squared_error(y_train, rf_train_predictions)
rf_test_mse = mean_squared_error(y_test, rf_test_predictions)

print("Decision Tree Errors:")
print(f"Train MAE: {dt_train_mae}, Test MAE: {dt_test_mae}")
print(f"Train MSE: {dt_train_mse}, Test MSE: {dt_test_mse}\n")

print("Random Forest Errors:")
print(f"Train MAE: {rf_train_mae}, Test MAE: {rf_test_mae}")
print(f"Train MSE: {rf_train_mse}, Test MSE: {rf_test_mse}")